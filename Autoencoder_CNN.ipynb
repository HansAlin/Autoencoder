{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpuutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b57177d03971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgpuutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGpuUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mGpuUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpuutils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gpuutils import GpuUtils\n",
    "GpuUtils.allocate(gpu_count=1, framework='keras')\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c95b3941e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#import seaborn as sns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from keras_flops import get_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing_data(signal, noise):\n",
    "  \"\"\"\n",
    "    This function normalize the data using mean and standard\n",
    "    deviation from noise data\n",
    "  \"\"\"\n",
    "  std = np.std(noise)\n",
    "  mean = np.mean(noise)\n",
    "  normalized_noise = (noise - mean)/std\n",
    "  normalized_signal = (signal - mean)/std\n",
    "  return normalized_noise, normalized_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'Data/trimmed100_data_noise_3.6SNR_1ch_0000.npy'\n",
    "noise = np.load(DATA_URL)\n",
    "\n",
    "for i in range(1,10):\n",
    "  noise = np.vstack((noise,np.load(f'Data/trimmed100_data_noise_3.6SNR_1ch_000{i}.npy')))\n",
    "\n",
    "noise = np.vstack((noise,np.load('Data/trimmed100_data_noise_3.6SNR_1ch_0010.npy')))\n",
    "signal = np.load(\"Data/trimmed100_data_signal_3.6SNR_1ch_0000.npy\")\n",
    "signal = np.vstack((signal,np.load(\"Data/trimmed100_data_signal_3.6SNR_1ch_0001.npy\")))\n",
    "n_classes = 2\n",
    "\n",
    "noise, signal = normalizing_data(signal, noise)\n",
    "shuffle = np.arange(noise.shape[0], dtype=np.int)\n",
    "np.random.shuffle(shuffle)\n",
    "noise = noise[shuffle]\n",
    "shuffle = np.arange(signal.shape[0], dtype=np.int)\n",
    "np.random.shuffle(shuffle)\n",
    "signal = signal[shuffle]\n",
    "\n",
    "number_of_test_samples = 20000\n",
    "signal_test = signal[:number_of_test_samples]\n",
    "noise_test = noise[:number_of_test_samples]\n",
    "signal = signal[number_of_test_samples:]\n",
    "noise = noise[number_of_test_samples:]\n",
    "\n",
    "\n",
    "x_test = np.vstack((noise_test, signal_test))\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "y_test = np.ones(len(x_test))\n",
    "y_test[:len(noise_test)] = 0\n",
    "shuffle = np.arange(x_test.shape[0])  #, dtype=np.int\n",
    "np.random.shuffle(shuffle)\n",
    "x_test = x_test[shuffle]\n",
    "y_test = y_test[shuffle]\n",
    "smask_test = y_test == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(signal, noise, signal_ratio=0.001, test_run=False ):\n",
    "  \"\"\"\n",
    "    This function creates training(validation) and test data based on choosen \n",
    "    signal ratio in sample.\n",
    "    Parameters:\n",
    "      signal = samples with signal\n",
    "      noise = samples with noise\n",
    "      test_run = creates a small training batch just for testing rest of code\n",
    "    Return:\n",
    "      x_train, smask_train, y_train, x_test, smask_test, y_test\n",
    "      \n",
    "  \"\"\"\n",
    "  \n",
    "  mini_batch_size = 10000\n",
    "  number_of_noise_samples = np.size(noise[:,0])\n",
    "\n",
    "  if test_run:\n",
    "    noise_train = noise[:mini_batch_size]\n",
    "    signal_train = signal[:mini_batch_size]\n",
    "  else:  \n",
    "    noise_train = noise\n",
    "    number_of_train_signals = np.floor((number_of_noise_samples-number_of_test_samples)*signal_ratio).astype(int)\n",
    "    signal_train = signal[:number_of_train_signals]\n",
    "\n",
    "\n",
    "\n",
    "  x_train = np.vstack((noise_train, signal_train))\n",
    "  x_train = np.expand_dims(x_train, axis=-1)\n",
    "  y_train = np.ones(len(x_train))\n",
    "  y_train[:len(noise_train)] = 0\n",
    "  shuffle = np.arange(x_train.shape[0]) #, dtype=np.int\n",
    "  np.random.shuffle(shuffle)\n",
    "  x_train = x_train[shuffle]\n",
    "  y_train = y_train[shuffle]\n",
    "  smask_train = y_train == 1\n",
    "\n",
    "  \n",
    "\n",
    "  return x_train, smask_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a few signal events\n",
    "# TODO \n",
    "def plot_signal_nois(x,smask):\n",
    "  for trace in x[smask][:2]:\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(trace)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # plot a few noise events\n",
    "  for noise in x[~smask][:2]:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(noise)\n",
    "    fig.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input, kernel=3, latent_space=6, filter_size=128, layers=2):\n",
    "  layer = input\n",
    "  maxpooling_size = 2\n",
    "  for i in range(layers):\n",
    "    layer = Conv1D(filter_size, kernel , activation='relu', padding='same')(layer)\n",
    "    if i ==2:\n",
    "      maxpooling_size = 5\n",
    "    else:\n",
    "      maxpooling_size = 2  \n",
    "    layer = MaxPooling1D(maxpooling_size, padding='same' )(layer)\n",
    "\n",
    "  layer = Flatten()(layer)\n",
    "  layer = Dense(latent_space)(layer)\n",
    "\n",
    "  encoded = layer\n",
    "\n",
    "  return encoded  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input, kernel=3, latent_space=6, filter_size=128, layers=2):\n",
    "  layer = input\n",
    "  data_size = len(x_test[0])\n",
    "  \n",
    "  size_of_first_feature = np.int(data_size / 2**layers)\n",
    "  if layers == 3:\n",
    "    size_of_first_feature = np.int(data_size / (2*2*5))\n",
    "  \n",
    "  upsampling_size = 2\n",
    "  \n",
    "  layer = Dense((filter_size * size_of_first_feature),activation='relu')(layer)\n",
    "  layer = Reshape((size_of_first_feature,filter_size))(layer)\n",
    "\n",
    "  for i in range(layers):\n",
    "    layer = Conv1D(filter_size, kernel, activation='relu', padding='same')(layer)\n",
    "    if layers == 3 and i == 0:\n",
    "      upsampling_size = 5\n",
    "    else:\n",
    "      upsampling_size = 2  \n",
    "    layer = UpSampling1D(upsampling_size)(layer)\n",
    "\n",
    "  layer = Conv1D(1, kernel, activation='tanh', padding='same', name='layer9')(layer)\n",
    "  \n",
    "  return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(input, kernel, latent_space, filter_size, layers):\n",
    "  enc = encoder(input, kernel, latent_space, filter_size, layers)\n",
    "  autoencoder = decoder(enc, kernel, latent_space, filter_size, layers )\n",
    "  return autoencoder\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder_model(data, kernel, latent_space,  filter_size, layers, learning_rate=0.0005,):\n",
    "\n",
    "  adam = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  input_data = keras.Input(shape=data[1].shape, name='first_layer')\n",
    "  \n",
    "  model = keras.Model(inputs=input_data, outputs=autoencoder(input_data, kernel, latent_space, filter_size=filter_size, layers=layers))\n",
    "  model.compile(\n",
    "      loss = 'mse',\n",
    "      optimizer = adam,\n",
    "      metrics = ['mse','mae','mape']   \n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc import callbacks\n",
    "def  train_autoencoder(model, x_train, epochs=50, batch=16, verbose=0):\n",
    "  early_stopping = keras.callbacks.EarlyStopping(\n",
    "                                    monitor=\"mse\",\n",
    "                                    min_delta=0,\n",
    "                                    patience=5,\n",
    "                                    verbose=0,\n",
    "                                    mode=\"auto\",\n",
    "                                    baseline=None,\n",
    "                                    restore_best_weights=True,\n",
    "                                )\n",
    "\n",
    "  val_split = 0.2\n",
    "  autoencoder = model.fit(x = x_train,\n",
    "                          y = x_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size = batch,\n",
    "                          verbose=verbose,\n",
    "                          shuffle = True,\n",
    "                          validation_split = val_split,\n",
    "                          callbacks=early_stopping\n",
    "                          )\n",
    "  return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(path, trained_autoencoder):\n",
    "  loss = trained_autoencoder.history['loss']\n",
    "  val_loss = trained_autoencoder.history['val_loss']\n",
    "\n",
    "  epochs = range(len(loss))\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.yscale('log')\n",
    "  plt.legend()\n",
    "  path = path + '_loss_plot.png'\n",
    "  plt.savefig(path)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(x):\n",
    "  for item in x:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(item)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_loss_values(model, x, smask):\n",
    "  \n",
    "  data_bins = np.size(x[0])\n",
    "  x_noise = x[~smask]\n",
    "  x_pred_noise = model.predict(x_noise)\n",
    "  x_signal = x[smask]\n",
    "  x_pred_signal = model.predict(x_signal)\n",
    "  noise_loss = keras.losses.mean_squared_error(x_noise, x_pred_noise)\n",
    "  noise_loss = np.sum(noise_loss, axis=1)/data_bins     #Per sample bin\n",
    "\n",
    "  signal_loss = keras.losses.mean_squared_error(x_signal, x_pred_signal)\n",
    "  signal_loss = np.sum(signal_loss, axis=1)/data_bins\n",
    "\n",
    "  return signal_loss, noise_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(path, signal_loss, noise_loss, resolution=100, plot=True):\n",
    "  max_value = np.max(signal_loss)\n",
    "  min_value = np.min(noise_loss)\n",
    "  low_lim = np.floor(np.log10(min_value))\n",
    "  high_lim = np.floor(np.log10(max_value))\n",
    "  bins = np.logspace(low_lim,high_lim , resolution)\n",
    "\n",
    "  if plot:\n",
    "    \n",
    "    ax1 = plt.hist(noise_loss, bins=bins, log=True, alpha=0.5, density=True)\n",
    "    ax2 = plt.hist(signal_loss, bins=bins, log=True, alpha=0.5, density=True)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Mean squared error')\n",
    "    plt.ylabel('Counts')\n",
    "    path = path + '_hist.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "  return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(path, signal_loss, noise_loss, bins,fpr=0.05, plot=True):\n",
    "  \"\"\"\n",
    "    This function takes signal and noise loss as arguments. They are \n",
    "    arrays from mse calculating.\n",
    "    Bins is taken from hist plot\n",
    "    Returns:\n",
    "      * the thershold value for a specific False Positive Ratio fpr\n",
    "      * True positive ratio, tpr\n",
    "      * False positive ratio, fpr\n",
    "      * True negative ratio, tnr\n",
    "      * False negative ratio, fnr\n",
    "\n",
    "  \"\"\"\n",
    "  min_value = np.min(signal_loss)\n",
    "  max_value = np.max(signal_loss)\n",
    "\n",
    "  thresholds = bins\n",
    "  threshold_value = 0\n",
    "  true_pos = np.zeros(len(bins))\n",
    "  false_pos = np.zeros(len(bins))\n",
    "  i = 0\n",
    "\n",
    "  tpr = 0\n",
    "  for limit in thresholds:\n",
    "    true_pos[i] = np.count_nonzero(signal_loss > limit)/len(signal_loss)\n",
    "    false_pos[i] =np.count_nonzero(noise_loss > limit)/len(noise_loss)\n",
    "    if false_pos[i-1] > fpr :\n",
    "      threshold_value = limit\n",
    "      tpr = true_pos[i]\n",
    "\n",
    "    i += 1\n",
    "\n",
    "  fnr = 1 - tpr\n",
    "  tnr = 1 - fpr  \n",
    "  \n",
    "\n",
    "  if plot:\n",
    "    plt.plot(false_pos,true_pos)  \n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.title('ROC')\n",
    "\n",
    "    ## Vertical line at False Positive Rate limit\n",
    "    y = np.linspace(0,1,2)\n",
    "    x = [fpr]*2\n",
    "    plt.plot(x,y)\n",
    "\n",
    "    plt.grid()\n",
    "    path = path + '_roc.png'\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "  return threshold_value, tpr, fpr, tnr, fnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_reduction_curve(path, signal_loss, noise_loss, fpr=0.05, plot=True):\n",
    "  \"\"\"\n",
    "    This function takes signal and noise loss as arguments. They are \n",
    "    arrays from mse calculating.\n",
    "    Bins is taken from hist plot\n",
    "    Returns:\n",
    "      * the thershold value for a specific False Positive Ratio fpr\n",
    "      * True positive ratio, tpr\n",
    "      * False positive ratio, fpr\n",
    "      * True negative ratio, tnr\n",
    "      * False negative ratio, fnr\n",
    "\n",
    "  \"\"\"\n",
    "  max_value = np.max(signal_loss)\n",
    "  min_value = np.min(noise_loss)\n",
    "  low_lim = np.floor(np.log10(min_value))\n",
    "  high_lim = np.floor(np.log10(max_value))\n",
    "  bins = np.logspace(low_lim,high_lim , 1000)\n",
    "\n",
    "  \n",
    "  threshold_value = 0\n",
    "  true_pos = np.zeros(len(bins))\n",
    "  false_pos = np.zeros(len(bins))\n",
    "  true_neg = np.zeros(len(bins))\n",
    "  false_neg = np.zeros(len(bins))\n",
    "  noise_reduction_factor = np.zeros(len(bins))\n",
    "\n",
    "\n",
    "  tpr = 0\n",
    "  for i, limit in enumerate(bins):\n",
    "    \n",
    "    true_pos[i] = np.count_nonzero(signal_loss > limit)/len(signal_loss)\n",
    "    false_pos[i] =np.count_nonzero(noise_loss > limit)/len(noise_loss)\n",
    "    true_neg[i] = 1 - false_pos[i]\n",
    "    false_neg[i] = 1 - true_pos[i]\n",
    "    \n",
    "\n",
    "    if (true_neg[i] < 1):\n",
    "      noise_reduction_factor[i] = 1 / ( 1 - true_neg[i])\n",
    "    else:\n",
    "      noise_reduction_factor[i] = len(noise_loss)  \n",
    "    \n",
    "    \n",
    "    if false_pos[i] > fpr:\n",
    "      threshold_value = limit\n",
    "      tpr = true_pos[i]\n",
    "\n",
    "\n",
    "  fnr = 1 - tpr\n",
    "  tnr = 1 - fpr  \n",
    "\n",
    "  if plot:\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "    ax.plot(true_pos,noise_reduction_factor)  \n",
    "    ax.set_ylabel(f'Noise reduction factor. Total {len(noise_loss)} noise events')\n",
    "    ax.set_xlabel('Efficiency/True Positive Rate')\n",
    "    ax.set_title('Signal efficiency vs. noise reduction factor')\n",
    "    ax.semilogy(True)\n",
    "    ax.set_xlim(0.875,1)\n",
    "    ax.grid()\n",
    "    path = path + '_Signal_efficiency_vs_noise_reduction_factor.png'\n",
    "    plt.savefig(path)\n",
    "    \n",
    "    plt.show()\n",
    "    # plt.plot(false_pos, true_pos)\n",
    "    # plt.show()\n",
    "  return threshold_value, tpr, fpr, tnr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "# * Confusion matrix\n",
    "# * pic the threshold value for a specific FPR\n",
    "# * Hyper paramters \n",
    "# * bach\n",
    "# * epochs\n",
    "# * learning rate\n",
    "# * ratio of signal / noise  events\n",
    "# * \n",
    "# * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(threshold_value, tpr, fpr, tnr, fnr):\n",
    "  from tabulate import tabulate\n",
    "\n",
    "  tabel = [['', 'Data-with-signals', 'Data-without-signal'],\n",
    "           ['Signal detected', f'{tpr:.2f}', fpr],\n",
    "           ['Noise detected', f'{fnr:.2f}', tnr]]\n",
    "  print(f'Confusion matrix with threshold value at {threshold_value:.2e}')  \n",
    "  print(tabulate(tabel, headers='firstrow'))         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(path,fpr):\n",
    "  results_path = path + '/' + 'results.csv'\n",
    "  results = pd.read_csv(results_path)\n",
    "\n",
    "  print(results)\n",
    "  column = results['True pos.']\n",
    "  index_of_max = column.idxmax()\n",
    "  best_model = results.iloc[index_of_max]  \n",
    "  \n",
    "\n",
    "  model_path = path + '/' + best_model['Model name'] + '.h5'\n",
    "  model = load_model(model_path)\n",
    "\n",
    "  signal_loss, noise_loss = prep_loss_values(model,x_test,smask_test)\n",
    "  bins = hist(path + '/' + 'best_model', signal_loss, noise_loss, plot=True)\n",
    "  threshold_value, tpr, fpr, tnr, fnr = noise_reduction_curve_multi_models([model], path+ '/' + 'best_model', fpr=fpr, plot=True)\n",
    "  confusion_matrix(threshold_value, tpr, fpr, tnr, fnr)\n",
    "  model.save((path + '/' + 'best_model'+ '.h5'))\n",
    "  print(best_model['False pos.'])\n",
    "  best_model['False pos.'] = fpr\n",
    "  print(best_model['False pos.'])\n",
    "  best_model['True pos.'] = tpr\n",
    "  print(best_model)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_reduction_curve_multi_models(models, path, fpr, plot=True):\n",
    "  \"\"\"\n",
    "    This function takes array of models as arguments. fpr value gives the\n",
    "    corresponding tpr\n",
    "    Bins is taken from hist plot\n",
    "    Returns:\n",
    "      * the thershold value for a specific False Positive Ratio fpr\n",
    "      * True positive ratio, tpr\n",
    "      * False positive ratio, fpr\n",
    "      * True negative ratio, tnr\n",
    "      * False negative ratio, fnr\n",
    "\n",
    "  \"\"\"\n",
    "  number_of_models = len(models)\n",
    "  results = [0]*number_of_models\n",
    "  for j in range(number_of_models):\n",
    "    \n",
    "    model = models[j]\n",
    "\n",
    "    signal_loss, noise_loss = prep_loss_values(model, x_test, smask_test)\n",
    "    \n",
    "    max_value = np.max(signal_loss)\n",
    "    min_value = np.min(noise_loss)\n",
    "    low_lim = np.floor(np.log10(min_value))\n",
    "    high_lim = np.floor(np.log10(max_value))\n",
    "    bins = np.logspace(low_lim,high_lim , 1000)\n",
    "\n",
    "\n",
    "    threshold_value = 0\n",
    "    true_pos = np.zeros(len(bins))\n",
    "    false_pos = np.zeros(len(bins))\n",
    "    true_neg = np.zeros(len(bins))\n",
    "    false_neg = np.zeros(len(bins))\n",
    "    noise_reduction_factor = np.zeros(len(bins))\n",
    "\n",
    "\n",
    "    tpr = 0\n",
    "    for i, limit in enumerate(bins):\n",
    "    \n",
    "      true_pos[i] = np.count_nonzero(signal_loss > limit)/len(signal_loss)\n",
    "      false_pos[i] =np.count_nonzero(noise_loss > limit)/len(noise_loss)\n",
    "      true_neg[i] = 1 - false_pos[i]\n",
    "      false_neg[i] = 1 - true_pos[i]\n",
    "    \n",
    "\n",
    "      if (true_neg[i] < 1):\n",
    "        noise_reduction_factor[i] = 1 / ( 1 - true_neg[i])\n",
    "      else:\n",
    "        noise_reduction_factor[i] = len(noise_loss)  \n",
    "      \n",
    "      \n",
    "      if false_pos[i] > fpr:\n",
    "        threshold_value = limit\n",
    "        tpr = true_pos[i]\n",
    "\n",
    "\n",
    "    fnr = 1 - tpr\n",
    "    tnr = 1 - fpr\n",
    "    \n",
    "\n",
    "    results[j] = [true_pos, true_neg, false_pos, false_neg, noise_reduction_factor]\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "  if plot:\n",
    "    for k in range(number_of_models):\n",
    "      model_name = 'model ' + str(k+1)\n",
    "      plt.plot(results[k][0],results[k][4], label=model_name)  \n",
    "      \n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel(f'Noise reduction factor. Total {len(x_test)/2} noise events')\n",
    "    plt.xlabel('Efficiency/True Positive Rate')\n",
    "    plt.title('Signal efficiency vs. noise reduction factor')\n",
    "    plt.semilogy(True)\n",
    "    plt.xlim(0.8,1)\n",
    "    plt.grid()\n",
    "    if len(models) > 1:\n",
    "      path = path + '/Signal_efficiency_vs_noise_reduction_factor_all_models.png'\n",
    "    else:\n",
    "      path = path + '_Signal_efficiency_vs_noise_reduction_factor.png'  \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "\n",
    "    plt.show()\n",
    "  # plt.plot(false_pos, true_pos)\n",
    "  # plt.show()\n",
    "\n",
    "  return threshold_value, tpr, fpr, tnr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(layers, model_number, latent_space, test_run, path, signal, noise, verbose, kernel, epochs=5, batch=256, learning_rate=0.0005, signal_ratio=1, plot=False, fpr=0.05, filter_size=128):\n",
    "  model_name = 'model_' + str(model_number)\n",
    "  print(model_name)\n",
    "  path = path + '/' + model_name\n",
    "\n",
    "  x_train, smask_train, y_train = create_data(signal, noise, signal_ratio=signal_ratio, test_run=False )\n",
    "  autoencoder_model = create_autoencoder_model(x_train, kernel=kernel, latent_space=latent_space, filter_size=filter_size, layers=layers, learning_rate=learning_rate )\n",
    "  autoencoder_model.summary()\n",
    "  #keras.utils.plot_model(autoencoder_model, to_file=(path + '.jpg'), show_layer_activations=True, show_dtype=True, show_shapes=True)\n",
    "  trained_autoencoder = train_autoencoder(autoencoder_model,x_train, epochs, batch, verbose)\n",
    "  signal_loss, noise_loss = prep_loss_values(autoencoder_model,x_test,smask_test)\n",
    "  if plot:\n",
    "    loss_plot(path, trained_autoencoder)\n",
    "  bins = hist(path, signal_loss, noise_loss, plot=plot)\n",
    "  threshold_value, tpr, fpr, tnr, fnr = noise_reduction_curve_multi_models([autoencoder_model],path, fpr=fpr, plot=plot)\n",
    "\n",
    "  flops = 0#get_flops(autoencoder_model)\n",
    "  #TODO get flops to work\n",
    "  autoencoder_model.save((path + '.h5'))\n",
    "\n",
    "  return model_name, epochs, batch, kernel, learning_rate, signal_ratio, fpr, tpr, threshold_value, latent_space, filter_size, flops, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(path, number_of_models):\n",
    "  models = []\n",
    "  for i in range(number_of_models):\n",
    "    model_path = path + '/' + 'model_' + str(i+1) + '.h5'\n",
    "    models.append(load_model(model_path))\n",
    "\n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "batches = [1024]\n",
    "learning_rates = [0.0001]\n",
    "signal_ratios = [0]\n",
    "kernels = [3]\n",
    "latent_spaces = [64,128,256]\n",
    "filters_size = [128]\n",
    "layers = [1]\n",
    "epochs = 300\n",
    "\n",
    "model_number = 7\n",
    "test_run = False\n",
    "plot =True\n",
    "fpr = 0.05\n",
    "verbose = 2\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=[ 'Model name', 'Epochs', 'Batch', 'Kernel', 'Learning rate', 'Signal ratio', 'False pos.', 'True pos.', 'Threshold value', 'Latent space', 'Filter size', 'Flops', 'Layers'])\n",
    "path = 'Models/complement_models'\n",
    "\n",
    "\n",
    "\n",
    "for batch in batches:\n",
    "  for learning_rate in learning_rates:\n",
    "    for signal_ratio in signal_ratios:\n",
    "      for kernel in kernels:\n",
    "        for latent_space in latent_spaces:\n",
    "          for filter_size in filters_size:\n",
    "            for layer in layers:\n",
    "              results.loc[model_number] = create_and_train_model(layers=layer,\n",
    "                                                               model_number=model_number,\n",
    "                                                              latent_space=latent_space,\n",
    "                                                              test_run=test_run,\n",
    "                                                              path=path,\n",
    "                                                              signal=signal,\n",
    "                                                              noise=noise,\n",
    "                                                              verbose=verbose,\n",
    "                                                              kernel=kernel,\n",
    "                                                              epochs=epochs,\n",
    "                                                              batch=batch,\n",
    "                                                              learning_rate=learning_rate,\n",
    "                                                              signal_ratio=signal_ratio, \n",
    "                                                              plot=plot,\n",
    "                                                              fpr=fpr,\n",
    "                                                              filter_size=filter_size)\n",
    "              model_number += 1\n",
    "\n",
    "\n",
    "results.to_csv(path + '/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_model(path,  fpr=0.05)\n",
    "models = load_models(path, 3)\n",
    "_ = noise_reduction_curve_multi_models(models,path, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'Models/complement_models/model_7.h5'\n",
    "# model_7 = load_model(path)\n",
    "# threshold, tpr, fpr, tnr,fnr = noise_reduction_curve_multi_models([model_7], path, fpr=0.05, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Models/CNN_003'\n",
    "find_best_model(path,  fpr=0.05)\n",
    "models = load_models(path, 9)\n",
    "_ = noise_reduction_curve_multi_models(models,path, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.load('Models/CNN_003/results.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c65829d28eb9a990a48d14c5d7ace61aefcd0accb8b94c1789c8db905031d9f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
