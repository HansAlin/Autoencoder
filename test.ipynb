{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1  will be allocated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gpuutils import GpuUtils\n",
    "GpuUtils.allocate(gpu_count=1, framework='keras')\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, UpSampling1D\n",
    "#from keras_flops import get_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing_data(data):\n",
    "  \"\"\"\n",
    "    This function normalize the data using mean and standard\n",
    "    deviation from noise data\n",
    "  \"\"\"\n",
    "  std = np.std(data)\n",
    "  mean = np.mean(data)\n",
    "  normalized_data = (data - mean)/std\n",
    "  \n",
    "  return normalized_data, std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalizing_data(normalized_data, std, mean):\n",
    "  data = normalized_data*std + mean\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(all_signals=True):\n",
    "  \"\"\"\n",
    "    This function loads data from ARIANNA group, downloaded localy\n",
    "    Args:\n",
    "     all_signals = True means that all the signals are\n",
    "    used in the test data. If all_signals = False only 20000 signals are used as test data.\n",
    "    Can be useful if training on signals aswell or just want to test data on small\n",
    "    test data.\n",
    "    Returns:\n",
    "      x_test, y_test, smask_test, signal, noise, std, mean\n",
    "    \n",
    "  \"\"\"\n",
    "  DATA_URL = '/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0000.npy'#/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0000.npy\n",
    "  noise = np.load(DATA_URL)\n",
    "\n",
    "  for i in range(1,10):\n",
    "    noise = np.vstack((noise,np.load(f'/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_000{i}.npy')))\n",
    "\n",
    "  noise = np.vstack((noise,np.load('/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0010.npy')))\n",
    "  signal = np.load(\"/home/halin/Autoencoder/Data/trimmed100_data_signal_3.6SNR_1ch_0000.npy\")\n",
    "  signal = np.vstack((signal,np.load(\"/home/halin/Autoencoder/Data/trimmed100_data_signal_3.6SNR_1ch_0001.npy\")))\n",
    "  n_classes = 2\n",
    "\n",
    "  signal, std, mean = normalizing_data(signal)\n",
    "  noise, std, mean = normalizing_data(noise)\n",
    "\n",
    "  shuffle = np.arange(noise.shape[0], dtype=np.int)\n",
    "  np.random.shuffle(shuffle)\n",
    "  noise = noise[shuffle]\n",
    "  shuffle = np.arange(signal.shape[0], dtype=np.int)\n",
    "  np.random.shuffle(shuffle)\n",
    "  signal = signal[shuffle]\n",
    "\n",
    "  number_of_test_samples = 0\n",
    "  if all_signals:\n",
    "    number_of_test_samples = len(signal)\n",
    "  else:  \n",
    "    number_of_test_samples = 20000\n",
    "    signal = signal[number_of_test_samples:]\n",
    "    noise = noise[number_of_test_samples:]\n",
    "\n",
    "  signal_test = signal[:number_of_test_samples]\n",
    "  noise_test = noise[:number_of_test_samples*2]\n",
    "  \n",
    "\n",
    "\n",
    "  x_test = np.vstack((noise_test, signal_test))\n",
    "  x_test = np.expand_dims(x_test, axis=-1)\n",
    "  y_test = np.ones(len(x_test))\n",
    "  y_test[:len(noise_test)] = 0\n",
    "  shuffle = np.arange(x_test.shape[0])  #, dtype=np.int\n",
    "  np.random.shuffle(shuffle)\n",
    "  x_test = x_test[shuffle]\n",
    "  y_test = y_test[shuffle]\n",
    "  smask_test = y_test == 1\n",
    "\n",
    "  return x_test, y_test, smask_test, signal, noise, std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test, smask_test, signal, noise, std, mean = load_data(all_signals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/halin/Autoencoder/Models/complement_models/best_model.h5'\n",
    "model = load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(x):\n",
    "  for item in x:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(item)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal(model, treshold, x, smask, under_treshold=True):\n",
    "  \"\"\"\n",
    "    This function steps trough the losses to find data tha are\n",
    "    below or above a sertain treshold.\n",
    "    Args:\n",
    "      model: keras model\n",
    "      treshold: (float) value to compare\n",
    "      x: data to test\n",
    "      smask: where the true signals are\n",
    "      under_treshold: (bool)\n",
    "    Returns:\n",
    "      outliers: the data beyond threshold in an list\n",
    "\n",
    "  \"\"\"\n",
    "  outliers = []\n",
    "  for i in range(len(x)):\n",
    "    x_pred = model.predict(np.array([x[i],]))\n",
    "    test = x[i]\n",
    "    \n",
    "    pred_loss = keras.losses.mean_squared_error(x[i], x_pred)\n",
    "    pred_loss = np.sum(pred_loss)/len(pred_loss)\n",
    "    if under_treshold:\n",
    "      if pred_loss < treshold:\n",
    "        outliers.append(x[i])\n",
    "        \n",
    "    else:\n",
    "      if pred_loss > treshold:\n",
    "        outliers.append(x[i])  \n",
    "  return outliers      \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = find_signal(model, 0.001, x_test, smask_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_models(path):\n",
    "  \"\"\"\n",
    "    This function search for keras models in an folder and loads\n",
    "    it to a list en returns a list of models. Models which contains \n",
    "    the substring \"best_model\" are excluded.\n",
    "    Args:\n",
    "      path: were to search for models\n",
    "  \"\"\"\n",
    "  \n",
    "  import glob\n",
    "  list_of_models = glob.glob(path + '/*.h5')\n",
    "  models = []\n",
    "  \n",
    "  for i, path in enumerate(list_of_models):\n",
    "    if 'best_model' in path:\n",
    "      pass\n",
    "    else: \n",
    "      models.append(load_model(path))\n",
    "\n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise reduction curve multi models\n",
    "def noise_reduction_curve_multi_models(models, path, fpr, plot=True, x_low_lim=0.8, save_outputs=True, models_to_plot=[]):\n",
    "  \"\"\"\n",
    "    This function takes signal and noise loss as arguments. They are \n",
    "    arrays from mse calculating.\n",
    "    Bins is taken from hist\n",
    "    Args: \n",
    "      models: a list of keras models\n",
    "      path: where the plots saves\n",
    "      fpr: False Positive Rate \n",
    "      x_low_lim: limit for lowest x value on plot (highest=1)\n",
    "    Returns:\n",
    "      thershold: value for a specific False Positive Ratio fpr for best model\n",
    "      tpr: True positive ratio for best model\n",
    "      fpr: False positive ratio for best model\n",
    "      tnr: True negative ratio for best model\n",
    "      fnr: False negative ratio for best model\n",
    "      results[0][4]: noise reduction factor for first model\n",
    "\n",
    "  \"\"\"\n",
    "  number_of_models = len(models)\n",
    "  results = [0]*number_of_models\n",
    "  for j in range(number_of_models):\n",
    "    \n",
    "    model = models[j]\n",
    "    not_found_treshold_value = True\n",
    "    signal_loss, noise_loss = prep_loss_values(model, x_test, smask_test)\n",
    "    \n",
    "    max_value = np.max(signal_loss)\n",
    "    min_value = np.min(noise_loss)\n",
    "    low_lim = np.floor(np.log10(min_value))\n",
    "    high_lim = np.floor(np.log10(max_value))\n",
    "    bins = np.logspace(low_lim,high_lim , 1000)\n",
    "\n",
    "\n",
    "    threshold_value = 0\n",
    "    true_pos = np.zeros(len(bins))\n",
    "    false_pos = np.zeros(len(bins))\n",
    "    true_neg = np.zeros(len(bins))\n",
    "    false_neg = np.zeros(len(bins))\n",
    "    noise_reduction_factor = np.zeros(len(bins))\n",
    "\n",
    "\n",
    "    tpr = 0\n",
    "    for i, limit in enumerate(bins):\n",
    "    \n",
    "      true_pos[i] = np.count_nonzero(signal_loss > limit)/len(signal_loss)\n",
    "      false_pos[i] =np.count_nonzero(noise_loss > limit)/len(noise_loss)\n",
    "      true_neg[i] = 1 - false_pos[i]\n",
    "      false_neg[i] = 1 - true_pos[i]\n",
    "    \n",
    "\n",
    "      if (true_neg[i] < 1):\n",
    "        noise_reduction_factor[i] = 1 / ( 1 - true_neg[i])\n",
    "      else:\n",
    "        noise_reduction_factor[i] = len(noise_loss)  \n",
    "      \n",
    "      \n",
    "      if false_pos[i] < fpr and not_found_treshold_value:\n",
    "        threshold_value = limit\n",
    "        tpr = true_pos[i]\n",
    "        not_found_treshold_value = False\n",
    "        \n",
    "\n",
    "\n",
    "    fnr = 1 - tpr\n",
    "    tnr = 1 - fpr\n",
    "    \n",
    "\n",
    "    results[j] = [true_pos, true_neg, false_pos, false_neg, noise_reduction_factor]\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "  if plot:\n",
    "    if len(models_to_plot) < 1:\n",
    "      models_to_plot = np.arange(0,len(models))\n",
    "\n",
    "    for k in models_to_plot:\n",
    "      model_name = 'model ' + str(k+1)\n",
    "      plt.plot(results[k][0],results[k][4], label=model_name)  \n",
    "      \n",
    "    noise_events = np.count_nonzero(~smask_test)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel(f'Noise reduction factor. Total {noise_events} noise events')\n",
    "    plt.xlabel('Efficiency/True Positive Rate')\n",
    "    plt.title('Signal efficiency vs. noise reduction factor')\n",
    "    plt.semilogy(True)\n",
    "    plt.xlim(x_low_lim,1)\n",
    "    plt.grid()\n",
    "    if len(models) > 1:\n",
    "      path = path + '/Signal_efficiency_vs_noise_reduction_factor_all_models.png'\n",
    "    else:\n",
    "      path = path + '_Signal_efficiency_vs_noise_reduction_factor.png'  \n",
    "    plt.tight_layout()\n",
    "    if save_outputs:\n",
    "      plt.savefig(path)\n",
    "\n",
    "    plt.show()\n",
    "  # plt.plot(false_pos, true_pos)\n",
    "  # plt.show()\n",
    "\n",
    "  return threshold_value, tpr, fpr, tnr, fnr, results[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/halin/Autoencoder/Models/CNN_002'\n",
    "models = load_models(path)\n",
    "_ = "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f00e6b041018f9c5003ba88af84c1401696fe75920157f0e0f441a09854937f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
