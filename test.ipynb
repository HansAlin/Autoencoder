{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0  will be allocated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gpuutils import GpuUtils\n",
    "GpuUtils.allocate(gpu_count=1, framework='keras')\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, UpSampling1D\n",
    "#from keras_flops import get_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing_data(data):\n",
    "  \"\"\"\n",
    "    This function normalize the data using mean and standard\n",
    "    deviation from noise data\n",
    "  \"\"\"\n",
    "  std = np.std(data)\n",
    "  mean = np.mean(data)\n",
    "  normalized_data = (data - mean)/std\n",
    "  \n",
    "  return normalized_data, std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalizing_data(normalized_data, std, mean):\n",
    "  data = normalized_data*std + mean\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(all_signals=True):\n",
    "  \"\"\"\n",
    "    This function loads data from ARIANNA group, downloaded localy\n",
    "    Args:\n",
    "     all_signals = True means that all the signals are\n",
    "    used in the test data. If all_signals = False only 20000 signals are used as test data.\n",
    "    Can be useful if training on signals aswell or just want to test data on small\n",
    "    test data.\n",
    "    Returns:\n",
    "      x_test, y_test, smask_test, signal, noise, std, mean\n",
    "    \n",
    "  \"\"\"\n",
    "  DATA_URL = '/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0000.npy'#/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0000.npy\n",
    "  noise = np.load(DATA_URL)\n",
    "\n",
    "  for i in range(1,10):\n",
    "    noise = np.vstack((noise,np.load(f'/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_000{i}.npy')))\n",
    "\n",
    "  noise = np.vstack((noise,np.load('/home/halin/Autoencoder/Data/trimmed100_data_noise_3.6SNR_1ch_0010.npy')))\n",
    "  signal = np.load(\"/home/halin/Autoencoder/Data/trimmed100_data_signal_3.6SNR_1ch_0000.npy\")\n",
    "  signal = np.vstack((signal,np.load(\"/home/halin/Autoencoder/Data/trimmed100_data_signal_3.6SNR_1ch_0001.npy\")))\n",
    "  n_classes = 2\n",
    "\n",
    "  signal, std, mean = normalizing_data(signal)\n",
    "  noise, std, mean = normalizing_data(noise)\n",
    "\n",
    "  shuffle = np.arange(noise.shape[0], dtype=np.int)\n",
    "  np.random.shuffle(shuffle)\n",
    "  noise = noise[shuffle]\n",
    "  shuffle = np.arange(signal.shape[0], dtype=np.int)\n",
    "  np.random.shuffle(shuffle)\n",
    "  signal = signal[shuffle]\n",
    "\n",
    "  number_of_test_samples = 0\n",
    "  if all_signals:\n",
    "    number_of_test_samples = len(signal)\n",
    "  else:  \n",
    "    number_of_test_samples = 20000\n",
    "    signal = signal[number_of_test_samples:]\n",
    "    noise = noise[number_of_test_samples:]\n",
    "\n",
    "  signal_test = signal[:number_of_test_samples]\n",
    "  noise_test = noise[:number_of_test_samples*2]\n",
    "  \n",
    "\n",
    "\n",
    "  x_test = np.vstack((noise_test, signal_test))\n",
    "  x_test = np.expand_dims(x_test, axis=-1)\n",
    "  y_test = np.ones(len(x_test))\n",
    "  y_test[:len(noise_test)] = 0\n",
    "  shuffle = np.arange(x_test.shape[0])  #, dtype=np.int\n",
    "  np.random.shuffle(shuffle)\n",
    "  x_test = x_test[shuffle]\n",
    "  y_test = y_test[shuffle]\n",
    "  smask_test = y_test == 1\n",
    "\n",
    "  return x_test, y_test, smask_test, signal, noise, std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test, smask_test, signal, noise, std, mean = load_data(all_signals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/halin/Autoencoder/Models/complement_models/best_model.h5'\n",
    "model = load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(x):\n",
    "  for item in x:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(item)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal(model, treshold, x, smask, under_treshold=True):\n",
    "  \"\"\"\n",
    "    This function steps trough the losses to find data tha are\n",
    "    below or above a sertain treshold.\n",
    "    Args:\n",
    "      model: keras model\n",
    "      treshold: (float) value to compare\n",
    "      x: data to test\n",
    "      smask: where the true signals are\n",
    "      under_treshold: (bool)\n",
    "    Returns:\n",
    "      outliers: the data beyond threshold in an list\n",
    "\n",
    "  \"\"\"\n",
    "  outliers = []\n",
    "  for i in range(len(x)):\n",
    "    x_pred = model.predict(np.array([x[i],]))\n",
    "    test = x[i]\n",
    "    \n",
    "    pred_loss = keras.losses.mean_squared_error(x[i], x_pred)\n",
    "    pred_loss = np.sum(pred_loss)/len(pred_loss)\n",
    "    if under_treshold:\n",
    "      if pred_loss < treshold:\n",
    "        outliers.append(x[i])\n",
    "        \n",
    "    else:\n",
    "      if pred_loss > treshold:\n",
    "        outliers.append(x[i])  \n",
    "  return outliers      \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = find_signal(model, 0.001, x_test, smask_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(outliers))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f00e6b041018f9c5003ba88af84c1401696fe75920157f0e0f441a09854937f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
